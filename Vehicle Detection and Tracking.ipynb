{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    '''画方框。'''\n",
    "    # make a copy of the image\n",
    "    draw_img = np.copy(img)\n",
    "    # draw each bounding box on your image copy using cv2.rectangle()\n",
    "    # return the image copy with boxes drawn\n",
    "    for box in bboxes:\n",
    "        cv2.rectangle(draw_img,box[0],box[1],color,thick)\n",
    "    return draw_img # Change this line to return image copy with boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_hist(img, color_space = 'RGB', nbins=32, bins_range=(0, 256)):\n",
    "    '''颜色的 histogram 特征'''\n",
    "    # Convert image to new color space (if specified)\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)  \n",
    "    # Compute the histogram of the RGB channels separately\n",
    "    rhist = np.histogram(feature_image[:,:,0], bins=32, range=(0, 256))\n",
    "    ghist = np.histogram(feature_image[:,:,1], bins=32, range=(0, 256))\n",
    "    bhist = np.histogram(feature_image[:,:,2], bins=32, range=(0, 256))\n",
    "    # Generating bin centers\n",
    "    bin_edges = rhist[1]\n",
    "    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return rhist, ghist, bhist, bin_centers, hist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, color_space='RGB', size=(32, 32)):\n",
    "    '''缩减图片的分辨率'''\n",
    "    # Convert image to new color space (if specified)\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)    \n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(feature_image,size).ravel()\n",
    "    # Return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hog_features(img, orient = 9, pix_per_cell = 8, cell_per_block = 2, vis=False, feature_vec=True):\n",
    "    '''获取 HOG 特征'''\n",
    "    if vis == True:\n",
    "        # Use skimage.hog() to get both features and a visualization\n",
    "        features, hog_image = hog(img,orient,(pix_per_cell,pix_per_cell),(cell_per_block,cell_per_block),visualise = vis, feature_vector= feature_vec) \n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        # Use skimage.hog() to get features only\n",
    "        features, _ = hog(img,orient,p(pix_per_cell,pix_per_cell),(cell_per_block,cell_per_block), visualise = vis)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_features(imgs, color_space = 'RGB')\n",
    "    '''合并特征'''\n",
    "    features = []\n",
    "    all_features = []\n",
    "    for img in imgs:\n",
    "        hog_features = get_hog_features(img)\n",
    "        _, _, _, _, color_features = color_hist(img)\n",
    "        features.append(np.concatenate([hog_features,color_features]))\n",
    "    # Fit a per-column scaler\n",
    "    if all_features = []:\n",
    "        all_features = features\n",
    "    all_features = np.vstack((all_features,features)).astype(np.float64)\n",
    "    X_scaler = StandardScaler().fit(all_features)\n",
    "    # Apply the scaler to X\n",
    "    scaled_all = X_scaler.transform(all_features)\n",
    "    return scaled_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "except:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data):\n",
    "    # rand_state = np.random.randint(0, 100)\n",
    "    rand_state = 42\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_search\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "def train_model(data,label,parameters = param_grid)\n",
    "    '''训练模型'''\n",
    "    svr = svm.SVC()\n",
    "    clf = grid_search.GridSearchCV(svr, parameters)\n",
    "    clf.fit(data,label)\n",
    "    model = clf\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    '''移动搜索的窗口'''\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Histogram of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Explain how you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Describe how you trained a classifier using your selected HOG features (and color features if you used them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Describe how you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Video Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Provide a link to your final video output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carnd",
   "language": "python",
   "name": "carnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
